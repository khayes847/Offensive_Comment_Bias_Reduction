{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dispatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swifter in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.295)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (5.4.5)\n",
      "Requirement already satisfied: numba in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (0.38.0)\n",
      "Requirement already satisfied: dask[complete]>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (2.6.0)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (0.24.2)\n",
      "Requirement already satisfied: parso>0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (4.36.1)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from swifter) (7.4.0)\n",
      "Requirement already satisfied: llvmlite>=0.23.0dev0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from numba->swifter) (0.23.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from numba->swifter) (1.17.3)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1; extra == \"complete\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dask[complete]>=0.19.0->swifter) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=0.5.1; extra == \"complete\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dask[complete]>=0.19.0->swifter) (0.5.2)\n",
      "Requirement already satisfied: distributed>=2.0; extra == \"complete\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dask[complete]>=0.19.0->swifter) (2.6.0)\n",
      "Requirement already satisfied: partd>=0.3.10; extra == \"complete\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dask[complete]>=0.19.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: bokeh>=1.0.0; extra == \"complete\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dask[complete]>=0.19.0->swifter) (1.0.4)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"complete\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from dask[complete]>=0.19.0->swifter) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.23.0->swifter) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=0.23.0->swifter) (2018.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipywidgets>=7.0.0->swifter) (4.8.2)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipywidgets>=7.0.0->swifter) (6.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipywidgets>=7.0.0->swifter) (3.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipywidgets>=7.0.0->swifter) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipywidgets>=7.0.0->swifter) (4.3.2)\n",
      "Requirement already satisfied: tornado>=5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (5.0.2)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (3.12)\n",
      "Requirement already satisfied: msgpack in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (0.6.0)\n",
      "Requirement already satisfied: click>=6.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (6.7)\n",
      "Requirement already satisfied: tblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (1.3.2)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (1.5.10)\n",
      "Requirement already satisfied: zict>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (0.1.3)\n",
      "Requirement already satisfied: locket in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from partd>=0.3.10; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (1.12.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (17.1)\n",
      "Requirement already satisfied: pillow>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (5.2.0)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bokeh>=1.0.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (2.10)\n",
      "Requirement already satisfied: jupyter_client in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (5.2.3)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (0.8.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (41.4.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (0.12.0)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (0.1.0)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (4.3.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (1.0.15)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (2.2.0)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (0.7.4)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (4.5.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (5.5.0)\n",
      "Requirement already satisfied: ipython_genutils in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (2.6.0)\n",
      "Requirement already satisfied: jupyter_core in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->swifter) (4.4.0)\n",
      "Requirement already satisfied: heapdict in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zict>=0.1.3->distributed>=2.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=16.8->bokeh>=1.0.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Jinja2>=2.7->bokeh>=1.0.0; extra == \"complete\"->dask[complete]>=0.19.0->swifter) (1.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jupyter_client->ipykernel>=4.5.1->ipywidgets>=7.0.0->swifter) (17.0.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->swifter) (0.5.2)\n",
      "Requirement already satisfied: nbconvert in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (5.4.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Send2Trash in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (0.8.1)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (0.8.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (0.2.3)\n",
      "Requirement already satisfied: bleach in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (2.1.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (1.4.2)\n",
      "Requirement already satisfied: testpath in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (0.3.1)\n",
      "Requirement already satisfied: defusedxml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (0.6.0)\n",
      "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (1.0.1)\n",
      "Requirement already satisfied: webencodings in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets>=7.0.0->swifter) (0.5.1)\n",
      "Collecting yellowbrick\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/cf/6d6ab47c0759d246262f9bdb53e89be3814bf1774bc51fffff995f5859f9/yellowbrick-1.0.1-py3-none-any.whl (378kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 3.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yellowbrick) (0.10.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yellowbrick) (1.1.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yellowbrick) (3.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yellowbrick) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from yellowbrick) (1.17.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cycler>=0.10.0->yellowbrick) (1.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.7.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (41.4.0)\n",
      "Installing collected packages: yellowbrick\n",
      "Successfully installed yellowbrick-1.0.1\n",
      "Collecting vaderSentiment\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 3.2MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.2.1\n",
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2MB 3.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/c0/25d19badc495428dec6a4bf7782de617ee0246a9211af75b302a2681dea7/smart_open-1.8.4.tar.gz (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.1MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim) (1.17.3)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.48.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.20.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.9.253)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.253 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.12.253)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.253->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.253->boto3->smart-open>=1.8.1->gensim) (2.7.3)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-1.8.4-cp36-none-any.whl size=68202 sha256=e744e75924399cdb5111dd6d80f0978f9a2ce72c30f48c450e858bd2483bd099\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5f/ea/fb/5b1a947b369724063b2617011f1540c44eb00e28c3d2ca8692\n",
      "Successfully built smart-open\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.1 smart-open-1.8.4\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "!pip install swifter\n",
    "!pip install yellowbrick\n",
    "!pip install vaderSentiment\n",
    "!pip install -U gensim\n",
    "import swifter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "import pickle \n",
    "import seaborn as sns\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "pd.set_option(\"max_columns\", 100)\n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "import data_prep as d\n",
    "import functions as f\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.17.3)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wrapt\n",
      "Installing collected packages: wrapt\n",
      "Successfully installed wrapt-1.11.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wrapt --upgrade --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.14.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.6.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.24.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow) (41.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn import preprocessing\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for comment in comment_list:\n",
    "    length.append(len(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(length)/len(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc0ecc6ac734815bf3bf1cfbbbe1e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99518d77504f40019a7755628ab4d7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835f367794f4440b851cc8d834b6659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2abc27ebac664f8da0d1df8f8fef4f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee5678b0ef843fbae127dceaec97cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eb9c0f9686410cbb9aa0e52d7fb261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d09ade73c90d4df18c4fa9ef79e59227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9259dd6eb2b433ea6766fcd3e5f0e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8969a4040bca4609846b858b83862ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d658a6359446479c78f07cb8ead729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3d0d25775e4626abba9c0591af8aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43dbca5054f4851beb5e6ec7354a80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57457680c7a490294f9f2bb1d425fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b165e231152043f2a6c5357e9f4c1a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdb4c40d3704357a95cde3b4bc482f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: word_tokenizer.tokenize(x.lower()))\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: ' '.join(x))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('i m ', 'i am ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('you re ', 'you are ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('he s ', 'he is ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('she s ', 'she is ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('it s ', 'it is ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('we re ', 'we are ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('they re ', 'they are ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('don t ', 'do not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('didn t ', 'did not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('doesn t ', 'does not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('wasn t ', 'was not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('weren t ', 'were not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('won t ', 'will not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('hasn t ', 'has not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('hadn t ', 'had not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('wouldn t ', 'would not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('i ll ', 'i will ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('he ll ', 'he will ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('she ll ', 'she will ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('you ll ', 'you will ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('we ll ', 'we will ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('they ll ', 'they will ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('aren t ', 'are not ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('i ve ', 'i have ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('you ve ', 'you have ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('we ve ', 'we have ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('they ve ', 'they have ')))\n",
    "data['comment_text'] = (data['comment_text'].swifter.apply(lambda x: x.replace('can t ', 'cannot ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d66d127197455e86ac971424aac45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa368e65f1049529ca238ac6487ba52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405130, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['target'] = data.target.swifter.apply(lambda x: 1 if x>=.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c512864ffc84cda85a8380ac2c8cdee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0669c418bf4e41d890daadcdd8e04451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7916306a18bf4fd1b90e19095f17d9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405109, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def ReplaceThreeOrMore(word_list):\n",
    "    \"\"\"Replaces letters that appear more than three times in a row\"\"\"\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    new_list = []\n",
    "    for word in word_list:\n",
    "        new_word = str(pattern.sub(r\"\\1\\1\", word))\n",
    "        new_list.append(new_word)\n",
    "    return new_list\n",
    "\n",
    "\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: ReplaceThreeOrMore(x))\n",
    "\n",
    "def repeat_words(x_list):\n",
    "    \"\"\"Changes some misspelled words\"\"\"\n",
    "    newlist = []\n",
    "    for x in x_list:\n",
    "        if x == 'bicthes':\n",
    "            newlist.append('bitches')\n",
    "        elif x == 'fukin':\n",
    "            newlist.append('fucking')\n",
    "        elif x == 'fectless':\n",
    "            newlist.append('feckless')\n",
    "        elif x == 'trustworhiness':\n",
    "            newlist.append('trustworthiness')\n",
    "        elif x == 'awfucl':\n",
    "            newlist.append('awful')\n",
    "        elif x == 'sextiimg':\n",
    "            newlist.append('sexting')\n",
    "        elif x == 'licemse':\n",
    "            newlist.append('license')\n",
    "        elif x == 'fking':\n",
    "            newlist.append('fucking')\n",
    "        elif x == 'fk':\n",
    "            newlist.append('fucking')\n",
    "        elif x == 'hizbollah':\n",
    "            newlist.append('hezbollah')\n",
    "        elif x == 'muticultural':\n",
    "            newlist.append('multicultural')\n",
    "        elif x == 'whatbdo':\n",
    "            newlist.append('what do')\n",
    "        elif x == 'mcinness':\n",
    "            newlist.append('mcinnes')\n",
    "        elif x == 'fundelmendalist':\n",
    "            newlist.append('fundamentalist')\n",
    "        elif x == 'cacusion':\n",
    "            newlist.append('caucasian')\n",
    "        elif x == 'puertoricans':\n",
    "            newlist.append('puerto ricans')\n",
    "        elif x == 'withba':\n",
    "            newlist.append('with a')\n",
    "        elif x == 'foxnew':\n",
    "            newlist.append('fox news')\n",
    "        elif x == 'blelow':\n",
    "            newlist.append('below')\n",
    "        elif x == 'fanclub':\n",
    "            newlist.append('fan club')\n",
    "        elif x == 'aswer':\n",
    "            newlist.append('answer')\n",
    "        else:\n",
    "            newlist.append(x)\n",
    "    return newlist\n",
    "\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: repeat_words(x))\n",
    "\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: ' '.join(x))\n",
    "\n",
    "data = d.whitespace(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: word_tokenizer.tokenize(x.lower()))\n",
    "\n",
    "common_words = ['a', 'the', 'with', 'without', 'in', 'on', 'of', 'or']\n",
    "\n",
    "phrases = Phrases(data['comment_text'], common_terms = common_words)\n",
    "\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "data['comment_text2'] = data['comment_text'].swifter.apply(lambda x: list(bigram[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.comment_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "data['comment_text3'] = data.comment_text.swifter.apply(lemmatize_text)\n",
    "\n",
    "data['comment_text3'] = data['comment_text'].swifter.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target_binary5.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_grouping(row):\n",
    "    val = 0\n",
    "    if row['no_group']==1:\n",
    "        if row['target']==0:\n",
    "            val = 0\n",
    "        elif row['target']==1:\n",
    "            val = 2\n",
    "    elif row['no_group']==0:\n",
    "        if row['target']==0:\n",
    "            val = 1\n",
    "        elif row['target']==1:\n",
    "            val = 3\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a325e911b0b449dbf479071b1212504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Pandas Apply', max=405130, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data['target'] = data.swifter.apply(lambda row: target_grouping(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns = ['target_binary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['target'])\n",
    "y = data[['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['target_binary4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.33,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_classifier(x_train, x_test, y_train, y_test):\n",
    "    \"\"\"Returns dummy classifier\"\"\"\n",
    "    print('Dummy')\n",
    "    y_train = y_train\n",
    "    y_test = y_test\n",
    "    dummy = DummyClassifier(strategy='most_frequent',\n",
    "                            random_state=42).fit(x_train, y_train)\n",
    "    y_pred = dummy.predict(x_test)\n",
    "    f.scores(y_test, y_pred)\n",
    "    f.c_matrix(dummy, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = X_train[['comment_text3']]\n",
    "X_test_text = X_test[['comment_text3']]\n",
    "\n",
    "clean_train_data = []\n",
    "for traindata in X_train_text['comment_text3']:\n",
    "    clean_train_data.append(traindata)\n",
    "\n",
    "clean_test_data = []\n",
    "for testdata in X_test_text['comment_text3']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.add('com')\n",
    "stopwords.add('www')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentlr = LogisticRegression(fit_intercept = False, solver='saga', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "C = np.logspace(0, 3, 10)\n",
    "hyperparameters = dict(C=C)\n",
    "clf = GridSearchCV(sentlr, hyperparameters, scoring='f1', cv=3, verbose=10, n_jobs=-2)\n",
    "best_model = clf.fit(np.array((X_train['compound_initial'])).reshape(-1,1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentlr = LogisticRegression(C=1, fit_intercept = False, solver='saga', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentlr.fit(np.array((X_train['compound_initial'])).reshape(-1,1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sentlr.predict(np.array((X_test['compound_initial'])).reshape(-1,1))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f.scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=stopwords,\n",
    "                             max_features=10000,\n",
    "                             ngram_range=(1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textreg = LogisticRegression(fit_intercept = False, solver='saga', random_state=42)\n",
    "C = np.logspace(0, 3, 10)\n",
    "hyperparameters = dict(C=C)\n",
    "clf = GridSearchCV(textreg, hyperparameters, scoring='f1', cv=3, verbose=10, n_jobs=-2)\n",
    "best_model = clf.fit(train_data_features, y_train)\n",
    "\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textreg = LogisticRegression(C=46.41588833612777, fit_intercept = False, solver='saga', random_state=42)\n",
    "\n",
    "textreg.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = textreg.predict(test_data_features)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['comment_text2'] = X_train['comment_text2'].swifter.apply(lambda x: ' '.join(x))\n",
    "X_test['comment_text2'] = X_test['comment_text2'].swifter.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = X_train[['comment_text2']]\n",
    "X_test_text = X_test[['comment_text2']]\n",
    "\n",
    "clean_train_data = []\n",
    "for traindata in X_train_text['comment_text2']:\n",
    "    clean_train_data.append(traindata)\n",
    "\n",
    "clean_test_data = []\n",
    "for testdata in X_test_text['comment_text2']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words=stopwords,\n",
    "                             max_features=10000,\n",
    "                             ngram_range=(1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "textreg = LogisticRegression(fit_intercept = False, solver='saga', random_state=42)\n",
    "C = np.logspace(0, 3, 10)\n",
    "hyperparameters = dict(C=C)\n",
    "clf = GridSearchCV(textreg, hyperparameters, scoring='f1', cv=3, verbose=10, n_jobs=-2)\n",
    "best_model = clf.fit(train_data_features, y_train)\n",
    "\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textreg = LogisticRegression(C=1, fit_intercept = False, solver='saga', random_state=42)\n",
    "\n",
    "textreg.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = textreg.predict(test_data_features)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.scores(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       haha you guys are a bunch of losers\n",
       "1         this is a great story man i wonder if the pers...\n",
       "2         angry trolls misogynists and racists oh my it ...\n",
       "3         nice to some attempts to try to make comments ...\n",
       "4         yet call out all muslims for the acts of a few...\n",
       "5         this bitch is nuts who would read a book by a ...\n",
       "6                                                   awesome\n",
       "7         yet another barack obama liberal media conspir...\n",
       "8         because the people who drive cars more are the...\n",
       "9         mormons have had a complicated relationship wi...\n",
       "10        i think you left out one very important organi...\n",
       "11        ah so part of the back end and algorithms auto...\n",
       "12        i d just ask how she intends to explain to min...\n",
       "13                                i am doing the same thing\n",
       "14        richard ellmyer check out agent bretzing s off...\n",
       "15        what is the attitude from the police about the...\n",
       "16        poverty does not make people more skilled in t...\n",
       "17        i think native americans should get a pass for...\n",
       "18           star wars did not come to burns until the 15th\n",
       "19        this is a fantastic idea assignment kids will ...\n",
       "20        creativity in the classroom is hard to come by...\n",
       "21        i think kate brown should move forward and imp...\n",
       "22        this sounds great i d never heard of straits c...\n",
       "23        good question jackie how do these yokels make ...\n",
       "24        will the protests have a specific call to acti...\n",
       "25        should they have to christians whether as a wh...\n",
       "26        will whiteness history month make all white pe...\n",
       "27        thank you for this article all i need to know ...\n",
       "28        read the whole article nowhere does it mention...\n",
       "29        to meet these people with threats of violence ...\n",
       "                                ...                        \n",
       "405100                                          where where\n",
       "405101    this is why elrod center which is a creative a...\n",
       "405102    i actually want the opposite for the females w...\n",
       "405103    no not at all i was told rather unceremoniousl...\n",
       "405104    man you are one strange person fixated on murd...\n",
       "405105    on thursday evening hannity devoted half of hi...\n",
       "405106    jesus expanded the 10 commandments from thou s...\n",
       "405107                    look at the socks what is he five\n",
       "405108    gardner was apparently looking for any excuse ...\n",
       "405109    no i expect the party who perpetuates the war ...\n",
       "405110    then you have zero empathy for victims nor und...\n",
       "405111    canadian prime minister justin trudeau acknowl...\n",
       "405112    good piece i am a member of a christian faith ...\n",
       "405113    yes for 2 years max i am talking ages 2 to 14 ...\n",
       "405114    judging by the hostility toward trudeau s care...\n",
       "405115    it is been trickling since last trump shellack...\n",
       "405116    like this ww crazydaysandnights net 2017 11 to...\n",
       "405117    this author makes no distinction between adult...\n",
       "405118    master asshole is what this guy was putting re...\n",
       "405119    i do not know that abortion played much of a p...\n",
       "405120    it does not matter when it is erected and for ...\n",
       "405121    women s rights last i checked women were just ...\n",
       "405122    every time there are testimonies like this i a...\n",
       "405123    xi and his comrades must be smirking over trum...\n",
       "405124    it is of course normal and natural for eugene ...\n",
       "405125    believing in god or not believing in god are p...\n",
       "405126    my thought exactly the only people he hasn t d...\n",
       "405127    i agree bill g the vote buying has begun by th...\n",
       "405128    no the probability of dying may be very very s...\n",
       "405129    nah i am too boring to parody this guy campbe ...\n",
       "Name: comment_text, Length: 405130, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[~(data.comment_text.isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 405109 entries, 0 to 405129\n",
      "Data columns (total 42 columns):\n",
      "id                                     405109 non-null int64\n",
      "target                                 405109 non-null int64\n",
      "cc_rejected                            405109 non-null int64\n",
      "cc_toxicity_annotator_count            405109 non-null int64\n",
      "cc_identity_annotator_count            405109 non-null int64\n",
      "cc_likes                               405109 non-null int64\n",
      "cc_disagree                            405109 non-null int64\n",
      "cc_funny                               405109 non-null int64\n",
      "cc_sad                                 405109 non-null int64\n",
      "cc_wow                                 405109 non-null int64\n",
      "comment_text                           405109 non-null object\n",
      "severe_toxicity                        405109 non-null float64\n",
      "obscene                                405109 non-null float64\n",
      "identity_attack                        405109 non-null float64\n",
      "insult                                 405109 non-null float64\n",
      "threat                                 405109 non-null float64\n",
      "sexual_explicit                        405109 non-null float64\n",
      "asian                                  405109 non-null int64\n",
      "atheist                                405109 non-null int64\n",
      "bisexual                               405109 non-null int64\n",
      "black                                  405109 non-null int64\n",
      "buddhist                               405109 non-null int64\n",
      "christian                              405109 non-null int64\n",
      "female                                 405109 non-null int64\n",
      "heterosexual                           405109 non-null int64\n",
      "hindu                                  405109 non-null int64\n",
      "homosexual_gay_or_lesbian              405109 non-null int64\n",
      "intellectual_or_learning_disability    405109 non-null int64\n",
      "jewish                                 405109 non-null int64\n",
      "latino                                 405109 non-null int64\n",
      "male                                   405109 non-null int64\n",
      "muslim                                 405109 non-null int64\n",
      "other_disability                       405109 non-null int64\n",
      "other_gender                           405109 non-null int64\n",
      "other_race_or_ethnicity                405109 non-null int64\n",
      "other_religion                         405109 non-null int64\n",
      "other_sexual_orientation               405109 non-null int64\n",
      "physical_disability                    405109 non-null int64\n",
      "psychiatric_or_mental_illness          405109 non-null int64\n",
      "transgender                            405109 non-null int64\n",
      "white                                  405109 non-null int64\n",
      "no_group                               405109 non-null int64\n",
      "dtypes: float64(6), int64(35), object(1)\n",
      "memory usage: 132.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 self._validate_apply(\n",
      "\u001b[0;32m<ipython-input-37-cf861bd4d059>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'lower'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-cf861bd4d059>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m    204\u001b[0m         ):  # if can't vectorize, estimate time to pandas apply\n\u001b[1;32m    205\u001b[0m             \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0mtimed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_REPEATS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0msample_proc_est\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimed\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_REPEATS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mest_apply_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_proc_est\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SAMPLE_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(stmt, setup, timer, number, globals)\u001b[0m\n\u001b[1;32m    231\u001b[0m            number=default_number, globals=None):\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m\"\"\"Convenience function to create Timer object and call timeit method.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m def repeat(stmt=\"pass\", setup=\"pass\", timer=default_timer,\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/timeit.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer, _stmt)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/swifter/swifter.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0msuppress_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SAMPLE_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3590\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3591\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3593\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-cf861bd4d059>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswifter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [haha, you, guys, are, a, bunch, of, losers]\n",
       "1         [this, is, a, great, story, man, i, wonder, if...\n",
       "2         [angry, trolls, misogynists, and, racists, oh,...\n",
       "3         [nice, to, some, attempts, to, try, to, make, ...\n",
       "4         [yet, call, out, all, muslims, for, the, acts,...\n",
       "5         [this, bitch, is, nuts, who, would, read, a, b...\n",
       "6                                                 [awesome]\n",
       "7         [yet, another, barack, obama, liberal, media, ...\n",
       "8         [because, the, people, who, drive, cars, more,...\n",
       "9         [mormons, have, had, a, complicated, relations...\n",
       "10        [i, think, you, left, out, one, very, importan...\n",
       "11        [ah, so, part, of, the, back, end, and, algori...\n",
       "12        [i, d, just, ask, how, she, intends, to, expla...\n",
       "13                         [i, am, doing, the, same, thing]\n",
       "14        [richard, ellmyer, check, out, agent, bretzing...\n",
       "15        [what, is, the, attitude, from, the, police, a...\n",
       "16        [poverty, does, not, make, people, more, skill...\n",
       "17        [i, think, native, americans, should, get, a, ...\n",
       "18        [star, wars, did, not, come, to, burns, until,...\n",
       "19        [this, is, a, fantastic, idea, assignment, kid...\n",
       "20        [creativity, in, the, classroom, is, hard, to,...\n",
       "21        [i, think, kate, brown, should, move, forward,...\n",
       "22        [this, sounds, great, i, d, never, heard, of, ...\n",
       "23        [good, question, jackie, how, do, these, yokel...\n",
       "24        [will, the, protests, have, a, specific, call,...\n",
       "25        [should, they, have, to, christians, whether, ...\n",
       "26        [will, whiteness, history, month, make, all, w...\n",
       "27        [thank, you, for, this, article, all, i, need,...\n",
       "28        [read, the, whole, article, nowhere, does, it,...\n",
       "29        [to, meet, these, people, with, threats, of, v...\n",
       "                                ...                        \n",
       "405100                                       [where, where]\n",
       "405101    [this, is, why, elrod, center, which, is, a, c...\n",
       "405102    [i, actually, want, the, opposite, for, the, f...\n",
       "405103    [no, not, at, all, i, was, told, rather, uncer...\n",
       "405104    [man, you, are, one, strange, person, fixated,...\n",
       "405105    [on, thursday, evening, hannity, devoted, half...\n",
       "405106    [jesus, expanded, the, 10, commandments, from,...\n",
       "405107           [look, at, the, socks, what, is, he, five]\n",
       "405108    [gardner, was, apparently, looking, for, any, ...\n",
       "405109    [no, i, expect, the, party, who, perpetuates, ...\n",
       "405110    [then, you, have, zero, empathy, for, victims,...\n",
       "405111    [canadian, prime, minister, justin, trudeau, a...\n",
       "405112    [good, piece, i, am, a, member, of, a, christi...\n",
       "405113    [yes, for, 2, years, max, i, am, talking, ages...\n",
       "405114    [judging, by, the, hostility, toward, trudeau,...\n",
       "405115    [it, is, been, trickling, since, last, trump, ...\n",
       "405116    [like, this, ww, crazydaysandnights, net, 2017...\n",
       "405117    [this, author, makes, no, distinction, between...\n",
       "405118    [master, asshole, is, what, this, guy, was, pu...\n",
       "405119    [i, do, not, know, that, abortion, played, muc...\n",
       "405120    [it, does, not, matter, when, it, is, erected,...\n",
       "405121    [women, s, rights, last, i, checked, women, we...\n",
       "405122    [every, time, there, are, testimonies, like, t...\n",
       "405123    [xi, and, his, comrades, must, be, smirking, o...\n",
       "405124    [it, is, of, course, normal, and, natural, for...\n",
       "405125    [believing, in, god, or, not, believing, in, g...\n",
       "405126    [my, thought, exactly, the, only, people, he, ...\n",
       "405127    [i, agree, bill, g, the, vote, buying, has, be...\n",
       "405128    [no, the, probability, of, dying, may, be, ver...\n",
       "405129    [nah, i, am, too, boring, to, parody, this, gu...\n",
       "Name: comment_text, Length: 405109, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sg_hs_10.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(X['comment_text'], size=300, window=10, min_count=1, workers=4, sg=1, hs=1)\n",
    "\n",
    "model.train(X['comment_text'], total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "model.wv.save_word2vec_format('sg_hs_10.txt', binary=False)\n",
    "joblib.dump(model, 'sg_hs_10.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(X['comment_text'], size=300, window=10, min_count=3, workers=4, sg=1, hs=1)\n",
    "\n",
    "model2.train(X['comment_text'], total_examples=model2.corpus_count, epochs=10)\n",
    "\n",
    "model2.wv.save_word2vec_format('sg_hs_10_mc3.txt', binary=False)\n",
    "joblib.dump(model2, 'sg_hs_10_mc3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Word2Vec(X['comment_text'], size=300, window=10, min_count=1, workers=4, sg=0, hs=1)\n",
    "\n",
    "model3.train(X['comment_text'], total_examples=model3.corpus_count, epochs=10)\n",
    "\n",
    "model3.wv.save_word2vec_format('sg_hs_10_cbow.txt', binary=False)\n",
    "joblib.dump(model3, 'sg_hs_10_cbow.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Word2Vec(X['comment_text'], size=300, window=10, min_count=3, workers=4, sg=0, hs=1)\n",
    "\n",
    "model4.train(X['comment_text'], total_examples=model4.corpus_count, epochs=10)\n",
    "\n",
    "model4.wv.save_word2vec_format('sg_hs_10_cbow_mc3.txt', binary=False)\n",
    "joblib.dump(model4, 'sg_hs_10_cbow_mc3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Word2Vec(X['comment_text'], size=300, window=13, min_count=1, workers=4, sg=1, hs=1)\n",
    "\n",
    "model5.train(X['comment_text'], total_examples=model5.corpus_count, epochs=10)\n",
    "\n",
    "model5.wv.save_word2vec_format('sg_hs_13.txt', binary=False)\n",
    "joblib.dump(model5, 'sg_hs_13.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Word2Vec(X['comment_text'], size=300, window=13, min_count=3, workers=4, sg=1, hs=1)\n",
    "\n",
    "model6.train(X['comment_text'], total_examples=model6.corpus_count, epochs=10)\n",
    "\n",
    "model6.wv.save_word2vec_format('sg_hs_13_mc3.txt', binary=False)\n",
    "joblib.dump(model6, 'sg_hs_13_mc3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model7 = Word2Vec(X['comment_text'], size=300, window=13, min_count=1, workers=4, sg=0, hs=1)\n",
    "\n",
    "model7.train(X['comment_text'], total_examples=model7.corpus_count, epochs=10)\n",
    "\n",
    "model7.wv.save_word2vec_format('cbow_hs_13.txt', binary=False)\n",
    "joblib.dump(model7, 'cbow_hs_13.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model8 = Word2Vec(X['comment_text'], size=300, window=13, min_count=3, workers=4, sg=0, hs=1)\n",
    "\n",
    "model8.train(X['comment_text'], total_examples=model8.corpus_count, epochs=10)\n",
    "\n",
    "model8.wv.save_word2vec_format('cbow_hs_13_mc3.txt', binary=False)\n",
    "joblib.dump(model8, 'cbow_hs_13_mc3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model9 = Word2Vec(X['comment_text'], size=300, window=16, min_count=1, workers=4, sg=1, hs=1)\n",
    "\n",
    "model9.train(X['comment_text'], total_examples=model9.corpus_count, epochs=10)\n",
    "\n",
    "model9.wv.save_word2vec_format('sg_hs_16.txt', binary=False)\n",
    "joblib.dump(model9, 'sg_hs_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = Word2Vec(X['comment_text'], size=300, window=16, min_count=3, workers=4, sg=1, hs=1)\n",
    "\n",
    "model10.train(X['comment_text'], total_examples=model10.corpus_count, epochs=10)\n",
    "\n",
    "model10.wv.save_word2vec_format('sg_hs_16_mc3.txt', binary=False)\n",
    "joblib.dump(model10, 'sg_hs_16_mc3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = Word2Vec(X['comment_text'], size=300, window=16, min_count=1, workers=4, sg=0, hs=1)\n",
    "\n",
    "model11.train(X['comment_text'], total_examples=model11.corpus_count, epochs=10)\n",
    "\n",
    "model11.wv.save_word2vec_format('cbow_hs_16.txt', binary=False)\n",
    "joblib.dump(model11, 'cbow_hs_16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12 = Word2Vec(X['comment_text'], size=300, window=16, min_count=3, workers=4, sg=0, hs=1)\n",
    "\n",
    "model12.train(X['comment_text'], total_examples=model12.corpus_count, epochs=10)\n",
    "\n",
    "model12.wv.save_word2vec_format('cbow_hs_16_mc3.txt', binary=False)\n",
    "joblib.dump(model12, 'cbow_hs_16_mc3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['comment_text'] = data['comment_text'].swifter.apply(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Word2Vec(X['comment_text'], size=300, window=5, min_count=1, workers=4, sg=1, hs=1)\n",
    "\n",
    "model3.train(X['comment_text'], total_examples=model3.corpus_count, epochs=10)\n",
    "\n",
    "model3.save_word2vec_format(model3.txt, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(X['comment_text'], size=300, window=5, min_count=1, workers=4, sg=1, hs=1)\n",
    "\n",
    "model2.train(X['comment_text'], total_examples=model2.corpus_count, epochs=10)\n",
    "\n",
    "model2.save_word2vec_format(model2.txt, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = joblib.load('wv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2.wv.most_similar('fuckin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save_word2vec_format('wv.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(X['comment_text'], size=200, window=7, min_count=1, workers=4, sg=0, hs=1)\n",
    "\n",
    "model2.train(X['comment_text'], total_examples=model2.corpus_count, epochs=10)\n",
    "\n",
    "wv2 = model2.wv\n",
    "\n",
    "joblib.dump(wv2, 'wv2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Word2Vec(X['comment_text'], size=100, window=12, min_count=1, workers=4, sg=1, hs=1)\n",
    "\n",
    "model3.train(X['comment_text'], total_examples=model3.corpus_count, epochs=10)\n",
    "\n",
    "wv3 = model3.wv\n",
    "\n",
    "joblib.dump(wv3, 'wv3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Word2Vec(X['comment_text'], size=100, window=12, min_count=1, workers=4, sg=0, hs=1)\n",
    "\n",
    "model4.train(X['comment_text'], total_examples=model4.corpus_count, epochs=10)\n",
    "\n",
    "wv4 = model4.wv\n",
    "\n",
    "joblib.dump(wv4, 'wv4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Word2Vec(X['comment_text'], size=100, window=12, min_count=1, workers=4, sg=0, hs=0, negative=10)\n",
    "\n",
    "model5.train(X['comment_text'], total_examples=model5.corpus_count, epochs=10)\n",
    "\n",
    "wv5 = model5.wv\n",
    "\n",
    "joblib.dump(wv5, 'wv5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Word2Vec(X['comment_text'], size=100, window=12, min_count=1, workers=4, sg=1, hs=0, negative=10)\n",
    "\n",
    "model6.train(X['comment_text'], total_examples=model6.corpus_count, epochs=10)\n",
    "\n",
    "wv6 = model6.wv\n",
    "\n",
    "joblib.dump(wv6, 'wv6.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(word for wordlist in data['comment_text'] for word in wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = set(word for wordlist in data['comment_text'] for word in wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('sg_hs_16_mc3.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in words:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2 = {}\n",
    "with open('model2.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in words2:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove2[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb =  Pipeline([(\"Word2Vec_Vectorizer\", W2vVectorizer(glove)),\n",
    "                (\"text_lr\", LogisticRegression(fit_intercept = False, solver='saga', random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [{'text_lr__C': np.logspace(0, 3, 10)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator=gb, param_grid=grid, scoring='f1', cv=3, n_jobs=-2, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch.fit(X_train['comment_text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best accuracy: %.3f' % gridsearch.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb =  Pipeline([(\"Word2Vec_Vectorizer\", W2vVectorizer(glove)),\n",
    "                (\"text_lr\", LogisticRegression(fit_intercept = False, solver='saga', C=215.44346900318823, \n",
    "                                               random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textreg.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2 =  Pipeline([(\"Word2Vec_Vectorizer\", W2vVectorizer(glove2)),\n",
    "                (\"Gradient_Boosting\", GradientBoostingClassifier(n_estimators=100, random_state=42, verbose=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = [{'Gradient_Boosting__min_samples_split': [2,4,6],\n",
    "         'Gradient_Boosting__min_samples_leaf': [1,3,5]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch2 = GridSearchCV(estimator=gb2, param_grid=grid2, scoring='f1', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch2.fit(X_train['comment_text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('AdaBoost', AdaBoostClassifier(random_state=42))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support_Vector_Classifier', SVC(random_state=42, verbose=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2 = [{'AdaBoost__n_estimators': [40, 50, 60],\n",
    "          'AdaBoost__algorithm': ['SAMME', 'SAMME.R']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch2 = GridSearchCV(estimator=ab, param_grid=grid2, scoring='f1', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch2.fit(X_train['comment_text2'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best accuracy: %.3f' % gridsearch2.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gridsearch2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3 = [{'Support_Vector_Classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "         'Support_Vector_Classifier__degree': [1,3,5],\n",
    "         'Support_Vector_Classifier__gamma': ['auto', 'scale']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch3 = GridSearchCV(estimator=svc, param_grid=grid3, scoring='f1', cv=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch3.fit(X_train['comment_text2'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best accuracy: %.3f' % gridsearch3.best_score_)\n",
    "\n",
    "# Best params\n",
    "print('\\nBest params:\\n', gridsearch3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensions of our coded results:', np.shape(one_hot_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences type: <class 'list'>\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (405109, 10000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-73982378fa70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sequences type:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mone_hot_results\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'one_hot_results type:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36mtexts_to_matrix\u001b[0;34m(self, texts, mode)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/keras_preprocessing/text.py\u001b[0m in \u001b[0;36msequences_to_matrix\u001b[0;34m(self, sequences, mode)\u001b[0m\n\u001b[1;32m    409\u001b[0m                              'before using tfidf mode.')\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (405109, 10000) and data type float64"
     ]
    }
   ],
   "source": [
    "comments = data[\"comment_text\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "tokenizer.fit_on_texts(comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "\n",
    "print('sequences type:', type(sequences))\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(comments, mode='binary')\n",
    "\n",
    "print('one_hot_results type:', type(one_hot_results))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "reverse_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "comment_idx_to_preview = 19\n",
    "print('Original complaint text:')\n",
    "print(comments[comment_idx_to_preview])\n",
    "print('\\n\\n')\n",
    "\n",
    "decoded_review = ' '.join([reverse_index.get(i) for i in sequences[comment_idx_to_preview]])\n",
    "\n",
    "print('Decoded review from Tokenizer:')\n",
    "print(decoded_review)\n",
    "\n",
    "target = data[\"target_binary\"]\n",
    "\n",
    "le = preprocessing.LabelEncoder() #Initialize. le used as abbreviation fo label encoder\n",
    "le.fit(target)\n",
    "print(\"Original class labels:\")\n",
    "print(list(le.classes_))\n",
    "print('\\n')\n",
    "target_cat = le.transform(target)  \n",
    "#list(le.inverse_transform([0, 1, 3, 3, 0, 6, 4])) #If you wish to retrieve the original descriptive labels post production\n",
    "\n",
    "print('New target labels:')\n",
    "print(target_cat)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('One hot labels; 2 binary columns, one for each of the categories.') #Each row will be all zeros except for the category for that observation.\n",
    "target_onehot = to_categorical(target_cat)\n",
    "print(target_onehot)\n",
    "print('\\n')\n",
    "\n",
    "print('One hot labels shape:')\n",
    "print(np.shape(target_onehot))\n",
    "\n",
    "len(one_hot_results)*.3\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "test_index = random.sample(range(0,405109), 121532)\n",
    "\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "\n",
    "\n",
    "label_test = target_onehot[test_index]\n",
    "label_train = np.delete(target_onehot, test_index, 0)\n",
    "\n",
    "print(\"Test label shape:\", np.shape(label_test))\n",
    "print(\"Train label shape:\", np.shape(label_train))\n",
    "print(\"Test shape:\", np.shape(test))\n",
    "print(\"Train shape:\", np.shape(train))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Dropout(.2))\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[f1_m, precision_m, recall_m])\n",
    "\n",
    "history = model.fit(train,\n",
    "                    label_train,\n",
    "                    epochs=120,\n",
    "                    batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(history, 'keras.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283577/283577 [==============================] - 14s 48us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3036083687181264,\n",
       " 0.8804618120193481,\n",
       " 0.8959149122238159,\n",
       " 0.8660684823989868]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_test = model.predict(test)\n",
    "results_train = model.evaluate(train, label_train)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121532/121532 [==============================] - 6s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2526585499670548,\n",
       " 0.6099817156791687,\n",
       " 0.6215859055519104,\n",
       " 0.5991604924201965]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test = model.evaluate(test, label_test)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
